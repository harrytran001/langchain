# Vector stores

<span data-heading-keywords="vector,vectorstore,vectorstores,vector store,vector stores"></span>

:::info[Prerequisites]

* [Embeddings](/docs/concepts/embedding_models/)
* [Text splitters](/docs/concepts/text_splitters/)

:::
:::info[Note]

This conceptual overview focuses on text-based indexing and retrieval for simplicity. 
However, embedding models can be [multi-modal](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings)
and vectorstores can be used to store and retrieve a variety of data types beyond text.
 
:::

## Overview

Vectorstores are a powerful and efficient way to index and retrieve unstructured data. 
They leverage vector [embeddings](/docs/concepts/embedding_models/), which are numerical representations of unstructured data that capture semantic meaning.
At their core, vectorstores utilize specialized data structures called vector indices. 
These indices are designed to perform efficient similarity searches over embedding vectors, allowing for rapid retrieval of relevant information based on semantic similarity rather than exact keyword matches.

## Key concepts

![Vectorstores](/img/vectorstores.png)

LangChain provides a universal interface for working with various vector store implementations. 

## Adding documents

There are [many different types of vectorstores](https://python.langchain.com/docs/integrations/vectorstores/).
LangChain provides a universal interface for working with them, providing a set of methods for common operations.
Using [Pinecone](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore) as an example, we initialize a vectorstore and supply the embedding model we want to use:

```python
from pinecone import Pinecone
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore

# Initialize Pinecone
pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"])

# Initialize with an embedding model
vector_store = PineconeVectorStore(index=pc.Index(index_name), embedding=OpenAIEmbeddings())
```

We can upsert text directly to the vectorstore, using the `add_texts` method.
In addition, we can use the `add_documents` method to upsert LangChain [Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html) objects to the vectorstore.
`Document` objects are a LangChain abstraction that contain text and metadata; the latter allows for filtering by metadata in some vectorstores.

```python
from langchain_core.documents import Document
document_1 = Document(
    page_content="I had chocalate chip pancakes and scrambled eggs for breakfast this morning.",
    metadata={"source": "tweet"},
)

document_2 = Document(
    page_content="The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.",
    metadata={"source": "news"},
)
documents = [document_1, document_2]
```

When we use the `add_documents` method, the vectorstore will use the embedding model to create an embedding of each document. 
Importantly, we also supply a unique identifier for each document. 
This gives us `upsert` functionality, which combines the functionality of inserting and updating records. 
If the record doesn't exist, it inserts a new record.
If the record already exists, it updates the existing record.
This method will `upsert` the embeddings and the documents in the vectorstore along with any associated metadata.

```python
# Given a list of documents and a vector store
uuids = [str(uuid4()) for _ in range(len(documents))]
vector_store.add_documents(documents=documents, ids=uuids)
```

:::info[Further reading]

* See the [full list of LangChain vectorstore integrations](/docs/integrations/vectorstores/).

:::

## Search

Vectorstores index the documents we add.
If we pass in a query, the vectorstore will return the most similar embedded documents.
Specifically, the vectorstore will embed the query, perform a similarity search over the embedded retrieval units, and return the most similar ones.
In order to perform the similarity search, it needs a way to measure the similarity between the query and the embedded retrieval units.
This is done using a similarity metric.

### Similarity metrics

A critical advange of embeddings vectors is they can be compared using many mathematical operations:

- **Cosine Similarity**: Measures the cosine of the angle between two vectors.
- **Euclidean Distance**: Measures the straight-line distance between two points.
- **Dot Product**: Measures the projection of one vector onto another.

:::tip

The choice of similarity metric can sometimes be selected, depending on the vectorstore implementation.
As an example, Pinecone allows the user to select the [similarity metric on index creation](/docs/integrations/vectorstores/pinecone/#initialization).

```python
pc.create_index(
        name=index_name,
        dimension=3072,
        metric="cosine",
    )
```

:::

### Similarity Search

Given a similarity metric to measure the distance between embedded query and embeddings documents, we need an algorithm to efficiently *find nearest neighbors* in high-dimensional spaces.
Many vectorstores implement [HNSW (Hierarchical Navigable Small World)](https://www.pinecone.io/learn/series/faiss/hnsw/), a graph-based index structure that allows for efficient similarity search.
Regardless of the search algorithm, the LangChain vectorstore interface has a `similarity_search` method for all integrations. 
This will take the income search query, create an embedding, and then find similar documents.

```python
query = "my query"
docs = vectorstore.similarity_search(query)
print(docs[0].page_content)
```

Many vectorstores allows various parameters to control the search process.
As an example, [the `k` parameter controls the number of results to return](/docs/integrations/vectorstores/pinecone/#query-directly).

:::info[Further reading]

* See the [how-to guide](/docs/how_to/vectorstores/) for more details on how to use the `similarity_search` method.
* See the [integrations page](/docs/integrations/vectorstores/) for more details on arguments that can be passed in to the `similarity_search` method for specific vectorstores.

:::

### Metadata filtering

Most vector stores can also store metadata about embedded vectors and support filtering on that metadata before
similarity search, allowing you more control over returned documents. This allows for querying in two different ways:

1. **Semantic search**: Query the unstructured data directly, often using via embedding or keyword similarity.
2. **Metadata search**: Apply structured query to the metadata, filering specific documents.

Vectorstore support for metadata is highly dependent on the underlying vector store implementation.
Many vectorstores support LangChain [Self Query](/docs/how_to/self_query/) retriever interface to perform metadata search using natural language.
Here is example usage with [Pinecone](/docs/integrations/vectorstores/pinecone/#query-directly):

```python
results = vector_store.similarity_search(
    "LangChain provides abstractions to make working with LLMs easy",
    k=2,
    filter={"source": "tweet"},
)
```  

## Advanced search and retreival techniques

While indexing algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.
For example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.
As a second example, some [vector stores](/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches. 
At the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.
See this [how-to guide](/docs/how_to/hybrid/) for more details.

| Name              | When to use                                              | Description                                                                                                                                                                            |
|-------------------|----------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Hybrid search](/docs/integrations/retrievers/pinecone_hybrid_search/)     | When combining keyword-based and semantic similarity.    | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. [Paper](https://arxiv.org/abs/2210.11934).                                                                               |
| [Maximal Marginal Relevance (MMR)](/docs/integrations/vectorstores/pinecone/#maximal-marginal-relevance-searches) | When needing to diversify search results. | MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.                                                                                  |

 